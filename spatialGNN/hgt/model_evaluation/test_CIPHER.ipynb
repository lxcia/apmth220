{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogeneous Graph Transformer Pretraining\n",
    "\n",
    "First, load relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PRETRAIN NODE EMBEDDING MODEL\n",
    "This script contains the main function for pretraining the node embedding model.\n",
    "'''\n",
    "\n",
    "# standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# import PyTorch and DGL\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "# import PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "# path manipulation\n",
    "from pathlib import Path\n",
    "\n",
    "# import project config file\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import project_config\n",
    "\n",
    "# custom imports\n",
    "# from hyperparameters import parse_args, get_hyperparameters\n",
    "# from dataloaders import load_graph, partition_graph, create_dataloaders\n",
    "# from models import HGT\n",
    "from utils import generate_subgraph\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "Define hyperparameters using code in `hyperparameters.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HYPERPARAMETERS\n",
    "\n",
    "This file contains the hyperparameters for the node embedder.\n",
    "'''\n",
    "\n",
    "# argument parser for command line arguments\n",
    "import argparse\n",
    "\n",
    "# COMMAND LINE ARGUMENTS REMOVED\n",
    "\n",
    "# PRE-TRAINING HYPERPARAMETERS\n",
    "def get_hyperparameters():   \n",
    "    '''\n",
    "    Return hyperparameters for node embedder. Combine tunable hyperparameters with fixed hyperparameters.\n",
    "    See parse_args() for all possible command line arguments.\n",
    "\n",
    "    Args:\n",
    "        args: command line arguments\n",
    "    \n",
    "    Tunable Parameters:\n",
    "        num_feat: dimension of embedding layer\n",
    "        num_heads: number of attention heads\n",
    "        hidden_dim: dimension of hidden layer\n",
    "        output_dim: dimension of output layer\n",
    "        wd: weight decay\n",
    "        dropout: dropout probability\n",
    "        lr: learning rate\n",
    "    '''\n",
    "\n",
    "    # generate dictionary from command-line arguments\n",
    "    # args_dict = vars(args)\n",
    "    args_dict = {\n",
    "        'node_list': project_config.NEUROKG_DIR / 'neuroKG_nodes.csv',\n",
    "        'edge_list': project_config.NEUROKG_DIR / 'neuroKG_edges.csv',\n",
    "        'save_dir': project_config.RESULTS_DIR / 'CIPHER' / 'pretraining',\n",
    "        'num_feat': 2048,\n",
    "        'num_heads': 4,\n",
    "        'hidden_dim': 32,\n",
    "        'output_dim': 128,\n",
    "        'wd': 0.0,\n",
    "        'dropout_prob': 0.3,\n",
    "        'lr': 0.0001,\n",
    "        'max_epochs': 250,\n",
    "        'resume': None,\n",
    "        'best_ckpt': None,\n",
    "        'save_embeddings': False,\n",
    "        'debug': True\n",
    "    }\n",
    "\n",
    "    # define fanout\n",
    "    fanout = [1, 1, 1] # [1, 1, 1]\n",
    "\n",
    "    # default hyperparameters\n",
    "    hparams_dict = {\n",
    "                # fixed parameters\n",
    "                'pred_threshold': 0.5,\n",
    "                'n_gpus': 1,\n",
    "                'num_workers': 4,\n",
    "                'train_batch_size': 1024, # 8\n",
    "                'val_batch_size': 1024,\n",
    "                'test_batch_size': 1024,\n",
    "                'sampler_fanout': fanout,\n",
    "                'num_layers': len(fanout),\n",
    "                'negative_k': 1,\n",
    "                'grad_clip': 1.0,\n",
    "                'lr_factor': 0.01,\n",
    "                'lr_patience': 100,\n",
    "                'lr_threshold': 1e-4,\n",
    "                'lr_threshold_mode': 'rel',\n",
    "                'lr_cooldown': 0,\n",
    "                'min_lr': 0,\n",
    "                'eps': 1e-8,\n",
    "                'seed': 42,\n",
    "                'profiler': None,\n",
    "                # see https://github.com/wandb/wandb/issues/714\n",
    "                'wandb_save_dir': project_config.RESULTS_DIR / 'wandb' / 'pretraining',\n",
    "                'log_every_n_steps': 10,\n",
    "                'time': False,\n",
    "                'sample_subgraph': False,\n",
    "                'seed_node': 1,\n",
    "                'n_walks': 100,\n",
    "                'walk_length': 10\n",
    "        }\n",
    "    \n",
    "    # combine tunable hyperparameters with fixed hyperparameters\n",
    "    hparams = dict(args_dict, **hparams_dict)\n",
    "    \n",
    "    print('Pre-Training Hyperparameters: ', hparams)\n",
    "    \n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders\n",
    "Define dataloaders using code in `dataloaders.py`. First, define the `load_graph()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD KNOWLEDGE GRAPH\n",
    "def load_graph(hparams):\n",
    "\n",
    "    # read in nodes and edges\n",
    "    # could also provide as args.node_list and args.edge_list with arg as argument of function\n",
    "    nodes = pd.read_csv(hparams['node_list'], dtype = {'node_index': int}, low_memory = False)\n",
    "    edges = pd.read_csv(hparams['edge_list'], dtype = {'edge_index': int, 'x_index': int, 'y_index': int}, low_memory = False)\n",
    "\n",
    "    # if sample subgraph, subsample nodes and edges\n",
    "    if hparams['sample_subgraph']:\n",
    "        nodes, edges = generate_subgraph(nodes, edges, hparams['seed_node'], hparams['n_walks'], hparams['walk_length'])\n",
    "        print(\"Number of subgraph nodes: \", len(nodes))\n",
    "        print(\"Number of subgraph edges: \", len(edges))\n",
    "\n",
    "    # group the nodes DataFrame by 'node_type' and use cumcount to generate the 'node_type_index'\n",
    "    nodes['node_type_index'] = nodes.groupby('node_type').cumcount()\n",
    "\n",
    "    # use the 'node_type_index' column to create the 'x_type_index' and 'y_type_index' columns in the edges DataFrame\n",
    "    edges['x_type_index'] = nodes.loc[edges['x_index'], 'node_type_index'].values\n",
    "    edges['y_type_index'] = nodes.loc[edges['y_index'], 'node_type_index'].values\n",
    "\n",
    "    # define empty dictionary to store graph data\n",
    "    neuroKG_data = {}\n",
    "\n",
    "    # group the edges DataFrame by unique combinations of x_type, relation, and y_type\n",
    "    grouped_edges = edges.groupby(['x_type', 'relation', 'y_type'], sort = False)\n",
    "\n",
    "    # iterate over the groups\n",
    "    for (x_type, relation, y_type), edges_subset in grouped_edges:\n",
    "\n",
    "        # convert edge indices to torch tensor\n",
    "        edge_indices = (torch.tensor(edges_subset['x_type_index'].values), torch.tensor(edges_subset['y_type_index'].values))\n",
    "\n",
    "        # add edge indices to data object\n",
    "        neuroKG_data[(x_type, relation, y_type)] = edge_indices\n",
    "\n",
    "        # print update\n",
    "        # print(f'Added edge relation: {x_type} - {relation} - {y_type}')\n",
    "\n",
    "    # instantiate a DGL HeteroGraph\n",
    "    neuroKG = dgl.heterograph(neuroKG_data)\n",
    "\n",
    "    # add node features to the heterograph\n",
    "    # first, group the nodes DataFrame by node_type\n",
    "    grouped_nodes = nodes.groupby('node_type', sort = False)\n",
    "\n",
    "    # iterate over the groups and add global node indices to the graph\n",
    "    for node_type, nodes_subset in grouped_nodes:\n",
    "\n",
    "        neuroKG.nodes[node_type].data['node_index'] = torch.tensor(nodes_subset['node_index'].values)\n",
    "\n",
    "    # return the graph\n",
    "    return neuroKG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `partition_graph()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTITION GRAPH\n",
    "def partition_graph(neuroKG, hparams):\n",
    "\n",
    "    # define dictionaries for train, validation, and test eids\n",
    "    train_eids = {}\n",
    "    val_eids = {}\n",
    "    test_eids = {}\n",
    "\n",
    "    # split the edges into train, validation, and test sets\n",
    "    forward_edge_types = [x for x in neuroKG.canonical_etypes if \"rev\" not in x[1]]\n",
    "    for etype in forward_edge_types:\n",
    "\n",
    "        # subset edge IDs for the current edge type\n",
    "        etype_eids = neuroKG.edges(etype = etype, form = 'eid')\n",
    "\n",
    "        # randomly shuffle edge IDs\n",
    "        num_edges = etype_eids.shape[0]\n",
    "        type_eids = etype_eids[torch.randperm(num_edges)]\n",
    "\n",
    "        # get train, validation, and test lengths\n",
    "        # here, we use a 80/15/5 split\n",
    "        test_length = int(np.ceil(0.05 * num_edges))\n",
    "        val_length = int(np.ceil(0.15 * num_edges))\n",
    "        train_length = num_edges - test_length - val_length\n",
    "\n",
    "        # split the edge IDs into train, validation, and test sets\n",
    "        etype_train_eids = etype_eids[:train_length]\n",
    "        etype_val_eids = etype_eids[train_length:(train_length + val_length)]\n",
    "        etype_test_eids = etype_eids[(train_length + val_length):]\n",
    "\n",
    "        # print number of edges in each set\n",
    "        # print(\"Edges of type {} split into {} train, {} validation, and {} test edges.\".format(etype, len(etype_train_eids), len(etype_val_eids), len(etype_test_eids)))\n",
    "\n",
    "        # add the edge IDs to the dictionaries\n",
    "        train_eids[etype] = etype_train_eids\n",
    "        val_eids[etype] = etype_val_eids\n",
    "        test_eids[etype] = etype_test_eids\n",
    "\n",
    "        # get reverse edge type\n",
    "        reverse_etype = (etype[2], \"rev_\" + etype[1], etype[0])\n",
    "\n",
    "        # add the reverse edge IDs to the dictionaries\n",
    "        train_eids[reverse_etype] = etype_train_eids\n",
    "        val_eids[reverse_etype] = etype_val_eids\n",
    "        test_eids[reverse_etype] = etype_test_eids\n",
    "    \n",
    "    # define new training graph\n",
    "    train_neuroKG = neuroKG.edge_subgraph(train_eids, relabel_nodes = False)\n",
    "\n",
    "    # combine train and validation edge IDs\n",
    "    train_val_eids = {}\n",
    "    for etype in train_eids.keys():\n",
    "        train_val_eids[etype] = torch.cat((train_eids[etype], val_eids[etype]))\n",
    "\n",
    "    # define new validation graph\n",
    "    val_neuroKG = neuroKG.edge_subgraph(train_val_eids, relabel_nodes = False)\n",
    "\n",
    "    # define new test graph\n",
    "    test_neuroKG = neuroKG.edge_subgraph(test_eids, relabel_nodes = False)\n",
    "\n",
    "    # return the graphs\n",
    "    return train_neuroKG, val_neuroKG, test_neuroKG, train_eids, val_eids, test_eids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new node sampler from `samplers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ..sampling.utils import EidExcluder\n",
    "# from .. import transforms\n",
    "# from ..base import NID\n",
    "# from .base import set_node_lazy_features, set_edge_lazy_features, Sampler\n",
    "from dgl.sampling.utils import EidExcluder\n",
    "from dgl.dataloading.shadow import set_node_lazy_features, set_edge_lazy_features, Sampler\n",
    "from collections import defaultdict\n",
    "\n",
    "class FixedSampler(Sampler):\n",
    "    \"\"\"Subgraph sampler that heterogeneous sampler that sets an upper \n",
    "    bound on the number of nodes included in each layer of the sampled subgraph.\n",
    "    \n",
    "    At each layer, the frontier is randomly subsampled. Rare node types can also be \n",
    "    upsampled by taking the scaled square root of the sampling probabilities.\n",
    "\n",
    "    It performs node-wise neighbor sampling and returns the subgraph induced by\n",
    "    all the sampled nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fanouts : list[int] or list[dict[etype, int]]\n",
    "        List of neighbors to sample per edge type for each GNN layer, with the i-th\n",
    "        element being the fanout for the i-th GNN layer.\n",
    "\n",
    "        If only a single integer is provided, DGL assumes that every edge type\n",
    "        will have the same fanout.\n",
    "\n",
    "        If -1 is provided for one edge type on one layer, then all inbound edges\n",
    "        of that edge type will be included.\n",
    "    fixed_k : int\n",
    "            The number of nodes to sample for each GNN layer.\n",
    "    upsample_rare_types : bool\n",
    "        Whether or not to upsample rare node types.\n",
    "    replace : bool, default True\n",
    "        Whether to sample with replacement\n",
    "    prob : str, optional\n",
    "        If given, the probability of each neighbor being sampled is proportional\n",
    "        to the edge feature value with the given name in ``g.edata``. The feature must be\n",
    "        a scalar on each edge.\n",
    "    \"\"\"\n",
    "    def __init__(self, fanouts, fixed_k, upsample_rare_types, replace=False, prob=None, \n",
    "                 prefetch_node_feats=None, prefetch_edge_feats=None, output_device=None):        \n",
    "        super().__init__()\n",
    "        self.fanouts = fanouts\n",
    "        self.replace = replace\n",
    "        self.fixed_k = fixed_k\n",
    "        self.upsample_rare_types = upsample_rare_types\n",
    "        self.prob = prob\n",
    "        self.prefetch_node_feats = prefetch_node_feats\n",
    "        self.prefetch_edge_feats = prefetch_edge_feats\n",
    "        self.output_device = output_device\n",
    "\n",
    "    def sample(self, g, seed_nodes, exclude_eids=None):\n",
    "        \"\"\"Sampling function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLGraph\n",
    "            The graph to sampler from.\n",
    "        seed_nodes : Tensor or dict[str, Tensor]\n",
    "            The nodes sampled in the current minibatch.\n",
    "        exclude_eids : Tensor or dict[etype, Tensor], optional\n",
    "            The edges to exclude from neighborhood expansion.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        input_nodes, output_nodes, subg\n",
    "            A triplet containing (1) the node IDs inducing the subgraph, (2) the node\n",
    "            IDs that are sampled in this minibatch, and (3) the subgraph itself.\n",
    "        \"\"\"\n",
    "\n",
    "        # define empty dictionary to store reached nodes\n",
    "        output_nodes = seed_nodes\n",
    "        all_reached_nodes = [seed_nodes]\n",
    "\n",
    "        # iterate over fanout\n",
    "        for fanout in reversed(self.fanouts):\n",
    "\n",
    "            # sample frontier\n",
    "            frontier = g.sample_neighbors(\n",
    "                seed_nodes, fanout, output_device=self.output_device,\n",
    "                replace=self.replace, prob=self.prob, exclude_edges=exclude_eids)\n",
    "\n",
    "            # get reached nodes\n",
    "            curr_reached = defaultdict(list)\n",
    "            for c_etype in frontier.canonical_etypes:\n",
    "                (src_type, rel_type, dst_type) = c_etype\n",
    "                src, _ = frontier.edges(etype = c_etype)\n",
    "                curr_reached[src_type].append(src)\n",
    "\n",
    "            # de-duplication\n",
    "            curr_reached = {ntype : torch.unique(torch.cat(srcs)) for ntype, srcs in curr_reached.items()}\n",
    "\n",
    "            # generate type sampling probabilties\n",
    "            type_count = {node_type: indices.shape[0] for node_type, indices in curr_reached.items()}\n",
    "            total_count = sum(type_count.values())\n",
    "            probs = {node_type: count / total_count for node_type, count in type_count.items()}\n",
    "\n",
    "            # upsample rare node types\n",
    "            if self.upsample_rare_types:\n",
    "\n",
    "                # take scaled square root of probabilities\n",
    "                prob_dist = list(probs.values())\n",
    "                prob_dist = np.sqrt(prob_dist)\n",
    "                prob_dist = prob_dist / prob_dist.sum()\n",
    "\n",
    "                # update probabilities\n",
    "                probs = {node_type: prob_dist[i] for i, node_type in enumerate(probs.keys())}\n",
    "\n",
    "            # generate node counts per type\n",
    "            n_per_type = {node_type: int(self.fixed_k * prob) for node_type, prob in probs.items()}\n",
    "            remainder = self.fixed_k - sum(n_per_type.values())\n",
    "            for _ in range(remainder):\n",
    "                node_type = np.random.choice(list(probs.keys()), p=list(probs.values()))\n",
    "                n_per_type[node_type] += 1\n",
    "\n",
    "            # downsample nodes\n",
    "            curr_reached_k = {}\n",
    "            for node_type, node_IDs in curr_reached.items():\n",
    "\n",
    "                # get number of total nodes and number to sample\n",
    "                num_nodes = node_IDs.shape[0]\n",
    "                n_to_sample = min(num_nodes, n_per_type[node_type])\n",
    "\n",
    "                # downsample nodes of current type\n",
    "                random_indices = torch.randperm(num_nodes)[:n_to_sample]\n",
    "                curr_reached_k[node_type] = node_IDs[random_indices]\n",
    "\n",
    "            # update seed nodes\n",
    "            seed_nodes = curr_reached_k\n",
    "            all_reached_nodes.append(curr_reached_k)\n",
    "\n",
    "        # merge all reached nodes before sending to DGLGraph.subgraph\n",
    "        merged_nodes = {}\n",
    "        for ntype in g.ntypes:\n",
    "            merged_nodes[ntype] = torch.unique(torch.cat([reached.get(ntype, []) for reached in all_reached_nodes]))\n",
    "        subg = g.subgraph(merged_nodes, relabel_nodes=True, output_device=self.output_device)\n",
    "\n",
    "        if exclude_eids is not None:\n",
    "            subg = EidExcluder(exclude_eids)(subg)\n",
    "\n",
    "        set_node_lazy_features(subg, self.prefetch_node_feats)\n",
    "        set_edge_lazy_features(subg, self.prefetch_edge_feats)\n",
    "\n",
    "        return seed_nodes, output_nodes, subg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the `create_dataloaders()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATA LOADERS\n",
    "def create_dataloaders(neuroKG, train_neuroKG, val_neuroKG, test_neuroKG,\n",
    "                       train_eids, val_eids, test_eids,\n",
    "                       sampler_fanout = [1, 1, 1], negative_k = 5,\n",
    "                       train_batch_size = 8, val_batch_size = 8, test_batch_size = 8,\n",
    "                       num_workers = 0):\n",
    "\n",
    "    print('Creating mini-batch pre-training dataloader...')\n",
    "\n",
    "    # define dictionary mapping forward edges to reverse edges, and vice versa\n",
    "    forward_edge_types = [x for x in neuroKG.canonical_etypes if \"rev\" not in x[1]]\n",
    "    reverse_edge_dict = {(u, r, v): (v, \"rev_\" + r, u) for u, r, v in forward_edge_types}\n",
    "    reverse_edge_dict.update({value: key for key, value in reverse_edge_dict.items()})\n",
    "\n",
    "    # define positive sampler\n",
    "    sampler = FixedSampler(sampler_fanout, fixed_k = 10, upsample_rare_types = True)\n",
    "\n",
    "    # other choices for positive sampler\n",
    "    # see https://docs.dgl.ai/en/latest/generated/dgl.dataloading.as_edge_prediction_sampler.html\n",
    "    # sampler = dgl.dataloading.MultiLayerFullNeighborSampler(3) # 3-layer full neighbor sampler\n",
    "    # sampler = dgl.dataloading.NeighborSampler([1, 1, 1]) # requires blocks\n",
    "    # sampler = dgl.dataloading.ShaDowKHopSampler(sampler_fanout)\n",
    "\n",
    "    # define negative sampler\n",
    "    # generate 5 negative samples per edge using uniform distribution\n",
    "    neg_sampler = dgl.dataloading.negative_sampler.Uniform(negative_k)\n",
    "    # define reverse edge types for each positive edge type and vice versa\n",
    "    \n",
    "    # convert to edge sampler\n",
    "    sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "        sampler,\n",
    "        exclude = \"reverse_types\", # exclude reverse edges\n",
    "        reverse_etypes = reverse_edge_dict, # define reverse edge types\n",
    "        negative_sampler = neg_sampler)\n",
    "\n",
    "    # define training dataloader\n",
    "    train_dataloader = dgl.dataloading.DataLoader(\n",
    "        train_neuroKG, train_eids, sampler,\n",
    "        batch_size = train_batch_size,\n",
    "        shuffle = True,\n",
    "        drop_last = False,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    # define validation dataloader\n",
    "    val_dataloader = dgl.dataloading.DataLoader(\n",
    "        val_neuroKG, val_eids, sampler,\n",
    "        batch_size = val_batch_size,\n",
    "        shuffle = True,\n",
    "        drop_last = False,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    # define test dataloader\n",
    "    test_dataloader = dgl.dataloading.DataLoader(\n",
    "        test_neuroKG, test_eids, sampler,\n",
    "        batch_size = test_batch_size,\n",
    "        shuffle = True,\n",
    "        drop_last = False,\n",
    "        num_workers = num_workers)\n",
    "    \n",
    "    # return the dataloaders\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Define the model using code in `models.py`. First, define the imports for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import DGL\n",
    "import dgl\n",
    "from dgl.nn.pytorch.conv import HGTConv\n",
    "\n",
    "# custom imports\n",
    "from utils import calculate_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the `BilinearDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BILINEAR DECODER CLASS\n",
    "class BilinearDecoder(pl.LightningModule): # overrides nn.Module\n",
    "\n",
    "    # INITIALIZATION\n",
    "    def __init__(self, num_etypes, embedding_dim):\n",
    "        '''\n",
    "        This function initializes a bilinear decoder.\n",
    "\n",
    "        Args:\n",
    "            num_etypes (int): Number of edge types.\n",
    "            embedding_dim (int): Dimension of embedding (i.e., output dimension * number of attention heads).\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # edge-type specific learnable weights\n",
    "        self.relation_weights = nn.Parameter(torch.Tensor(num_etypes, embedding_dim))\n",
    "\n",
    "        # initialize weights\n",
    "        nn.init.xavier_uniform_(self.relation_weights, gain = nn.init.calculate_gain('leaky_relu'))\n",
    "    \n",
    "\n",
    "    # ADD EDGE TYPE INDEX\n",
    "    def add_edge_type_index(self, edge_graph):\n",
    "        '''\n",
    "        This function adds an integer edge type label to each edge in the graph. This is required for the decoder.\n",
    "        Specifically, the edge type label is used to subset the right row of the relation weight matrix.\n",
    "        \n",
    "        Args:\n",
    "            edge_graph (dgl.DGLGraph): Positive or negative edge graph.\n",
    "        '''\n",
    "\n",
    "        # iterate over the canonical edge types\n",
    "        for edge_index, edge_type in enumerate(edge_graph.canonical_etypes):\n",
    "        \n",
    "            # get number of edges of that type\n",
    "            num_edges = edge_graph.num_edges(edge_type)\n",
    "\n",
    "            # add integer label to edge\n",
    "            edge_graph.edges[edge_type].data['edge_type_index'] = torch.tensor([edge_index] * num_edges, device = self.device) #.to(device)\n",
    "    \n",
    "    \n",
    "    # DECODER\n",
    "    def decode(self, edges):\n",
    "        '''\n",
    "        This is a user-defined function over the edges to generate the score for each edge.\n",
    "        See https://docs.dgl.ai/en/0.9.x/generated/dgl.DGLGraph.apply_edges.html.\n",
    "        '''\n",
    "        \n",
    "        # get source embeddings\n",
    "        src_embeddings = edges.src['node_embedding']\n",
    "        dst_embeddings = edges.dst['node_embedding']\n",
    "\n",
    "        # get relation weight for specific edge type\n",
    "        # note that, because the decode function is applied by edge type, we can use the first edge to get the edge type\n",
    "        edge_type_index = edges.data['edge_type_index'][0] # see torch.unique(edges.data['edge_type'])\n",
    "        rel_weights = self.relation_weights[edge_type_index]\n",
    "\n",
    "        # compute weighted dot product\n",
    "        # each row of src_embeddings is multiplied by rel_weights, then element-wise multiplied by dst_embeddings\n",
    "        # finally, a row-wise sum is performed to get a single score per edge\n",
    "        score = torch.sum(src_embeddings * rel_weights * dst_embeddings, dim = 1)\n",
    "\n",
    "        return {'score': score}\n",
    "\n",
    "\n",
    "    # COMPUTE SCORE\n",
    "    def compute_score(self, edge_graph):\n",
    "        '''\n",
    "        This function computes the score for positive or negative edges using dgl.DGLGraph.apply_edges.\n",
    "\n",
    "        Args:\n",
    "            edge_graph (dgl.DGLGraph): Positive or negative edge graph.\n",
    "        '''\n",
    "\n",
    "        with edge_graph.local_scope():\n",
    "\n",
    "            # get edge types with > 0 number of edges in the positive graph\n",
    "            nonzero_edge_types = [etype for etype in edge_graph.canonical_etypes if edge_graph.num_edges(etype) != 0]\n",
    "\n",
    "            # compute score for positive graph\n",
    "            for etype in nonzero_edge_types:\n",
    "                edge_graph.apply_edges(self.decode, etype = etype)\n",
    "            \n",
    "            # return scores\n",
    "            return edge_graph.edata['score']\n",
    "    \n",
    "    \n",
    "    # FORWARD PASS\n",
    "    def forward(self, subgraph, pos_graph, neg_graph, node_embeddings):\n",
    "        '''\n",
    "        This function performs a forward pass of the bilinear decoder.\n",
    "\n",
    "        Args:\n",
    "            subgraph (dgl.DGLHeteroGraph): Subgraph.\n",
    "            pos_graph (dgl.DGLHeteroGraph): Positive graph.\n",
    "            neg_graph (dgl.DGLHeteroGraph): Negative graph.\n",
    "            node_embeddings (torch.Tensor): Node embeddings.\n",
    "        '''\n",
    "\n",
    "        # get subgraph node IDs\n",
    "        subgraph_nodes = subgraph.ndata['node_index']\n",
    "\n",
    "        # assign node embeddings to positive and negative graphs\n",
    "        # iterate over node types in positive graph\n",
    "        for ntype in pos_graph.ntypes:\n",
    "\n",
    "            # get positive graph node IDs\n",
    "            pos_graph_nodes = pos_graph.ndata['node_index'][ntype].unsqueeze(1)\n",
    "            \n",
    "            # find indices of positive graph nodes in subgraph\n",
    "            # note, that indices are same for negative graph\n",
    "            # compare pos_graph.ndata['_ID'] vs. neg_graph.ndata['_ID']\n",
    "            pos_graph_indices = torch.where(subgraph_nodes == pos_graph_nodes)[1]\n",
    "\n",
    "            # add embeddings as feature to graph\n",
    "            pos_graph.nodes[ntype].data['node_embedding'] = node_embeddings[pos_graph_indices]\n",
    "            neg_graph.nodes[ntype].data['node_embedding'] = node_embeddings[pos_graph_indices]\n",
    "\n",
    "        # add edge indices to positive and negative graphs\n",
    "        self.add_edge_type_index(pos_graph)\n",
    "        self.add_edge_type_index(neg_graph)\n",
    "\n",
    "        # compute scores for positive and negative graphs\n",
    "        pos_graph_scores = self.compute_score(pos_graph)\n",
    "        neg_graph_scores = self.compute_score(neg_graph)\n",
    "\n",
    "        # return scores\n",
    "        return pos_graph_scores, neg_graph_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the `HGT` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HETEROGENEOUS GRAPH TRANSFORMER\n",
    "class HGT(pl.LightningModule):\n",
    "    \n",
    "    # INITIALIZATION\n",
    "    def __init__(self, num_nodes, num_ntypes, num_etypes, num_feat = 1024, num_heads = 4,\n",
    "                 hidden_dim = 256, output_dim = 128, num_layers = 2,\n",
    "                 dropout_prob = 0.5, pred_threshold = 0.5,\n",
    "                 lr = 0.0001, wd = 0.0, lr_factor = 0.01, lr_patience = 100, lr_threshold = 1e-4,\n",
    "                 lr_threshold_mode = 'rel', lr_cooldown = 0, min_lr = 1e-8, eps = 1e-8,\n",
    "                 hparams = None):\n",
    "        '''\n",
    "        This function initializes the model and defines the model hyperparameters and architecture.\n",
    "\n",
    "        Args:\n",
    "            num_nodes (int): Number of nodes in the graph.\n",
    "            num_ntypes (int): Number of node types in the graph.\n",
    "            num_etypes (int): Number of edge types in the graph.\n",
    "            num_feat (int): Number of input features (i.e., hidden embedding dimension).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            hidden_dim (int): Number of hidden units in the second to last HGT layer.\n",
    "            output_dim (int): Number of output units.\n",
    "            num_layers (int): Number of HGT layers.\n",
    "            dropout_prob (float): Dropout probability.\n",
    "            pred_threshold (float): Prediction threshold to compute metrics.\n",
    "            lr (float): Learning rate.\n",
    "            wd (float): Weight decay.\n",
    "            lr_factor (float): Factor by which to reduce learning rate.\n",
    "            lr_patience (int): Number of epochs with no improvement after which learning rate will be reduced.\n",
    "            lr_threshold (float): Threshold for measuring the new optimum, to only focus on significant changes.\n",
    "            lr_threshold_mode (str): One of ['rel', 'abs'].\n",
    "            lr_cooldown (int): Number of epochs to wait before resuming normal operation after lr reduction.\n",
    "            min_lr (float): A lower bound on the learning rate of all param groups or each group respectively.\n",
    "            eps (float): Term added to the denominator to improve numerical stability.\n",
    "            hparams (dict): Dictionary of model hyperparameters. Will override all other arguments if not None.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # if hparams_dict is None, construct dictionary from arguments\n",
    "        if hparams is None:\n",
    "            hparams = locals()\n",
    "\n",
    "        # save model hyperparameters\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.num_feat = hparams['num_feat']\n",
    "        self.num_heads = hparams['num_heads']\n",
    "        self.hidden_dim = hparams['hidden_dim']\n",
    "        self.output_dim = hparams['output_dim']\n",
    "        self.num_layers = hparams['num_layers']\n",
    "        self.dropout_prob = hparams['dropout_prob']\n",
    "        self.pred_threshold = hparams['pred_threshold']\n",
    "\n",
    "        # learning rate parameters\n",
    "        self.lr = hparams['lr']\n",
    "        self.wd = hparams['wd']\n",
    "        self.lr_factor = hparams['lr_factor']\n",
    "        self.lr_patience = hparams['lr_patience']\n",
    "        self.lr_threshold = hparams['lr_threshold']\n",
    "        self.lr_threshold_mode = hparams['lr_threshold_mode']\n",
    "        self.lr_cooldown = hparams['lr_cooldown']\n",
    "        self.min_lr = hparams['min_lr']\n",
    "        self.eps = hparams['eps']\n",
    "\n",
    "        # calculate sizes of hidden dimensions\n",
    "        self.h_dim_1 = hidden_dim * 2\n",
    "        self.h_dim_2 = hidden_dim\n",
    "\n",
    "        # define node embeddings\n",
    "        self.emb = nn.Embedding(num_nodes, num_feat)\n",
    "\n",
    "        # layer 1\n",
    "        self.conv1 = HGTConv(in_size = num_feat, head_size = self.h_dim_1, num_heads = num_heads,\n",
    "                                num_ntypes = num_ntypes, num_etypes = num_etypes, dropout = 0.2, use_norm = True)\n",
    "\n",
    "        # layer normalization 1\n",
    "        self.norm1 = nn.LayerNorm(self.h_dim_1 * num_heads)\n",
    "        \n",
    "        if self.num_layers == 2:\n",
    "        \n",
    "            # layer 2\n",
    "            self.conv2 = HGTConv(in_size = self.h_dim_1 * num_heads, head_size = output_dim, num_heads = num_heads,\n",
    "                                    num_ntypes = num_ntypes, num_etypes = num_etypes, dropout = 0.2, use_norm = True)\n",
    "            \n",
    "        elif self.num_layers == 3:\n",
    "        \n",
    "            # layer 2\n",
    "            self.conv2 = HGTConv(in_size = self.h_dim_1 * num_heads, head_size = self.h_dim_2, num_heads = num_heads,\n",
    "                                    num_ntypes = num_ntypes, num_etypes = num_etypes, dropout = 0.2, use_norm = True)\n",
    "\n",
    "            # layer normalization 2\n",
    "            self.norm2 = nn.LayerNorm(self.h_dim_2 * num_heads)\n",
    "\n",
    "            # layer 3\n",
    "            self.conv3 = HGTConv(in_size = self.h_dim_2 * num_heads, head_size = output_dim, num_heads = num_heads,\n",
    "                                    num_ntypes = num_ntypes, num_etypes = num_etypes, dropout = 0.2, use_norm = True)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            # raise error\n",
    "            raise ValueError('Number of layers must be 2 or 3.')\n",
    "\n",
    "        # define decoder\n",
    "        self.decoder = BilinearDecoder(num_etypes, output_dim * num_heads)\n",
    "        \n",
    "    \n",
    "    # FORWARD PASS\n",
    "    def forward(self, subgraph):\n",
    "        '''\n",
    "        This function performs a forward pass of the model. Note that the subgraph must be converted to from a \n",
    "        heterogeneous graph to homogeneous graph for efficiency.\n",
    "\n",
    "        Args:\n",
    "            subgraph (dgl.DGLHeteroGraph): Subgraph containing the nodes and edges for the current batch.\n",
    "        '''\n",
    "\n",
    "        # get global indices\n",
    "        global_node_indices = subgraph.ndata['node_index']\n",
    "\n",
    "        # get node embeddings from the first MFG layer\n",
    "        x = self.emb(global_node_indices)      \n",
    "\n",
    "        # pass node embedding through first two layers\n",
    "        x = self.conv1(subgraph, x, subgraph.ndata[dgl.NTYPE], subgraph.edata[dgl.ETYPE])\n",
    "        x = self.norm1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(subgraph, x, subgraph.ndata[dgl.NTYPE], subgraph.edata[dgl.ETYPE])\n",
    "\n",
    "        # check if 3 layers\n",
    "        if self.num_layers == 3:\n",
    "\n",
    "            # pass node embedding through layer 3\n",
    "            x = self.norm2(x)\n",
    "            x = F.leaky_relu(x)\n",
    "            x = self.conv3(subgraph, x, subgraph.ndata[dgl.NTYPE], subgraph.edata[dgl.ETYPE])\n",
    "        \n",
    "        # return node embeddings\n",
    "        return x\n",
    "    \n",
    "\n",
    "    # STEP FUNCTION USED FOR TRAINING, VALIDATION, AND TESTING\n",
    "    def _step(self, input_nodes, pos_graph, neg_graph, subgraph, mode):\n",
    "        '''Defines the step that is run on each batch of data. PyTorch Lightning handles steps including:\n",
    "            - Moving data to the correct device.\n",
    "            - Epoch and batch iteration.\n",
    "            - optimizer.step(), loss.backward(), optimizer.zero_grad() calls.\n",
    "            - Calling of model.eval(), enabling/disabling grads during evaluation.\n",
    "            - Logging of metrics.\n",
    "        \n",
    "        Args:\n",
    "            input_nodes (torch.Tensor): Input nodes.\n",
    "            pos_graph (dgl.DGLHeteroGraph): Positive graph.\n",
    "            neg_graph (dgl.DGLHeteroGraph): Negative graph.\n",
    "            subgraph (dgl.DGLHeteroGraph): Subgraph.\n",
    "            mode (str): The mode of the step (train, val, test).\n",
    "        '''\n",
    "\n",
    "        # get batch size by summing number of nodes in each node type\n",
    "        batch_size = sum([x.shape[0] for x in input_nodes.values()])\n",
    "\n",
    "        # convert heterogeneous graph to homogeneous graph for efficiency\n",
    "        # see https://docs.dgl.ai/en/latest/generated/dgl.to_homogeneous.html\n",
    "        subgraph = dgl.to_homogeneous(subgraph, ndata = ['node_index'])\n",
    "        \n",
    "        # send to GPU\n",
    "        # subgraph = subgraph.to(device)\n",
    "        # pos_graph = pos_graph.to(device)\n",
    "        # neg_graph = neg_graph.to(device)\n",
    "\n",
    "        # get node embeddings\n",
    "        node_embeddings = self.forward(subgraph)\n",
    "\n",
    "        # compute score from decoder\n",
    "        pos_scores, neg_scores = self.decoder(subgraph, pos_graph, neg_graph, node_embeddings)\n",
    "\n",
    "        # compute loss\n",
    "        loss, metrics = self.compute_loss(pos_scores, neg_scores)\n",
    "\n",
    "        # return loss and metrics\n",
    "        return loss, metrics, batch_size\n",
    "    \n",
    "\n",
    "    # TRAINING STEP\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''Defines the step that is run on each batch of training data.'''\n",
    "\n",
    "        # get batch elements\n",
    "        input_nodes, pos_graph, neg_graph, subgraph = batch\n",
    "\n",
    "        # get loss and metrics\n",
    "        loss, metrics, batch_size = self._step(input_nodes, pos_graph, neg_graph, subgraph, mode = 'train')\n",
    "\n",
    "        # log loss and metrics\n",
    "        values = {\"train/loss\": loss.detach(),\n",
    "                  \"train/accuracy\": metrics['accuracy'],\n",
    "                  \"train/ap\": metrics['ap'],\n",
    "                  \"train/f1\": metrics['f1'],\n",
    "                  \"train/auroc\": metrics['auroc']}\n",
    "        self.log_dict(values, batch_size = batch_size)\n",
    "\n",
    "        # return loss\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    # VALIDATION STEP\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''Defines the step that is run on each batch of validation data.'''\n",
    "\n",
    "        # get batch elements\n",
    "        input_nodes, pos_graph, neg_graph, subgraph = batch\n",
    "\n",
    "        # get loss and metrics\n",
    "        loss, metrics, batch_size = self._step(input_nodes, pos_graph, neg_graph, subgraph, mode = 'val')\n",
    "\n",
    "        # log loss and metrics\n",
    "        values = {\"val/loss\": loss.detach(),\n",
    "                  \"val/accuracy\": metrics['accuracy'],\n",
    "                  \"val/ap\": metrics['ap'],\n",
    "                  \"val/f1\": metrics['f1'],\n",
    "                  \"val/auroc\": metrics['auroc']}\n",
    "        self.log_dict(values, batch_size = batch_size)\n",
    "\n",
    "\n",
    "    # TEST STEP\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''Defines the step that is run on each batch of test data.'''\n",
    "\n",
    "        # get batch elements\n",
    "        input_nodes, pos_graph, neg_graph, subgraph = batch\n",
    "\n",
    "        # get loss and metrics\n",
    "        loss, metrics, batch_size = self._step(input_nodes, pos_graph, neg_graph, subgraph, mode = 'test')\n",
    "\n",
    "        # log loss and metrics\n",
    "        values = {\"test/loss\": loss.detach(),\n",
    "                  \"test/accuracy\": metrics['accuracy'],\n",
    "                  \"test/ap\": metrics['ap'],\n",
    "                  \"test/f1\": metrics['f1'],\n",
    "                  \"test/auroc\": metrics['auroc']}\n",
    "        self.log_dict(values, batch_size = batch_size)\n",
    "\n",
    "    \n",
    "    # LOSS FUNCTION\n",
    "    def compute_loss(self, pos_scores, neg_scores):\n",
    "        '''\n",
    "        This function computes the loss and metrics for the current batch.\n",
    "        '''\n",
    "\n",
    "        # concatenate positive and negative scores across edge types\n",
    "        pos_pred = torch.cat(list(pos_scores.values()))\n",
    "        neg_pred = torch.cat(list(neg_scores.values()))\n",
    "        raw_pred = torch.cat((pos_pred, neg_pred))\n",
    "\n",
    "        # transform with activation function\n",
    "        pred = torch.sigmoid(raw_pred)\n",
    "\n",
    "        # construct target vector\n",
    "        pos_target = torch.ones(pos_pred.shape[0])\n",
    "        neg_target = torch.zeros(neg_pred.shape[0])\n",
    "        target = torch.cat((pos_target, neg_target)).to(self.device) #.to(device)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy(pred, target, reduction = \"mean\")\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = calculate_metrics(pred.cpu().detach().numpy(), target.cpu().detach().numpy(), self.pred_threshold)\n",
    "        return loss, metrics\n",
    "    \n",
    "\n",
    "    # OPTIMIZER AND SCHEDULER\n",
    "    def configure_optimizers(self):\n",
    "        '''\n",
    "        This function is called by PyTorch Lightning to get the optimizer and scheduler.\n",
    "        We reduce the learning rate by a factor of lr_factor if the validation loss does not improve for lr_patience epochs.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the optimizer and scheduler.\n",
    "        '''\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay = self.wd)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode = 'min', factor = self.lr_factor, patience = self.lr_patience,\n",
    "            threshold = self.lr_threshold, threshold_mode = self.lr_threshold_mode,\n",
    "            cooldown = self.lr_cooldown, min_lr = self.min_lr, eps = self.eps\n",
    "        )\n",
    "        \n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val/loss\",\n",
    "                    'name': 'curr_lr'\n",
    "                    },\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Model\n",
    "\n",
    "Expand `pretrain()` function in `pretrain.py` to test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training Hyperparameters:  {'node_list': PosixPath('/n/data1/hms/dbmi/zitnik/lab/users/an252/NeuroKG/neuroKG/Data/NeuroKG/4_final_KG/neuroKG_nodes.csv'), 'edge_list': PosixPath('/n/data1/hms/dbmi/zitnik/lab/users/an252/NeuroKG/neuroKG/Data/NeuroKG/4_final_KG/neuroKG_edges.csv'), 'save_dir': PosixPath('/n/data1/hms/dbmi/zitnik/lab/users/an252/NeuroKG/neuroKG/Results/pretrain'), 'num_feat': 2048, 'num_heads': 4, 'hidden_dim': 32, 'output_dim': 128, 'wd': 0.0, 'dropout_prob': 0.3, 'lr': 0.0001, 'max_epochs': 250, 'resume': '', 'best_ckpt': None, 'save_embeddings': False, 'debug': True, 'pred_threshold': 0.5, 'n_gpus': 1, 'num_workers': 4, 'train_batch_size': 1024, 'val_batch_size': 1024, 'test_batch_size': 1024, 'sampler_fanout': [1, 1, 1], 'num_layers': 3, 'negative_k': 1, 'grad_clip': 1.0, 'lr_factor': 0.01, 'lr_patience': 100, 'lr_threshold': 0.0001, 'lr_threshold_mode': 'rel', 'lr_cooldown': 0, 'min_lr': 0, 'eps': 1e-08, 'seed': 42, 'profiler': None, 'wandb_save_dir': PosixPath('/n/data1/hms/dbmi/zitnik/lab/users/an252/NeuroKG/neuroKG/Results/wandb/pretrain'), 'log_every_n_steps': 10, 'time': False, 'sample_subgraph': False, 'seed_node': 1, 'n_walks': 100, 'walk_length': 10}\n",
      "Creating mini-batch pre-training dataloader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/data1/hms/dbmi/zitnik/lab/users/an252/NeuroKG/neuroKG/neuroKG_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# get hyperparameters\n",
    "# args = parse_args()\n",
    "hparams = get_hyperparameters() \n",
    "\n",
    "# set seed\n",
    "pl.seed_everything(hparams['seed'], workers = True)\n",
    "\n",
    "# load NeuroKG knowledge graph\n",
    "neuroKG = load_graph(hparams)\n",
    "\n",
    "# partition graph into train, validation, and test sets\n",
    "train_neuroKG, val_neuroKG, test_neuroKG, train_eids, val_eids, test_eids = partition_graph(neuroKG, hparams)\n",
    "\n",
    "# get dataloaders\n",
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(\n",
    "    neuroKG, train_neuroKG, val_neuroKG, test_neuroKG, train_eids, val_eids, test_eids,\n",
    "    sampler_fanout = hparams['sampler_fanout'], negative_k = hparams['negative_k'],\n",
    "    train_batch_size = hparams['train_batch_size'], val_batch_size = hparams['val_batch_size'], \n",
    "    test_batch_size = hparams['test_batch_size'], num_workers = hparams['num_workers']\n",
    ")\n",
    "\n",
    "# enable CPU affinity\n",
    "train_dataloader.enable_cpu_affinity()\n",
    "val_dataloader.enable_cpu_affinity()\n",
    "test_dataloader.enable_cpu_affinity()\n",
    "\n",
    "# instantiate logger\n",
    "curr_time = datetime.now()\n",
    "run_name = curr_time.strftime('%H:%M:%S on %m/%d/%Y')\n",
    "run_id = curr_time.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "# wandb_logger = WandbLogger(name = run_name, project = 'cipher-pretraining', entity = 'ayushnoori',\n",
    "#                             save_dir = hparams['wandb_save_dir'], id = run_id, resume = \"allow\")\n",
    "\n",
    "# instantiate models\n",
    "model = HGT(\n",
    "    num_nodes = train_neuroKG.num_nodes(), num_ntypes = len(train_neuroKG.ntypes),\n",
    "    num_etypes = len(train_neuroKG.canonical_etypes), hparams = hparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/data1/hms/dbmi/zitnik/lab/users/an252/NeuroKG/neuroKG/neuroKG_env/lib/python3.9/site-packages/dgl/dataloading/dataloader.py:1149: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n",
      "  dgl_warning(\n"
     ]
    }
   ],
   "source": [
    "# move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# going to be training_step function\n",
    "for input_nodes, pos_graph, neg_graph, subgraph in train_dataloader:\n",
    "\n",
    "    # convert heterogeneous graph to homogeneous graph for efficiency\n",
    "    # see https://docs.dgl.ai/en/latest/generated/dgl.to_homogeneous.html\n",
    "    subgraph = dgl.to_homogeneous(subgraph, ndata = ['node_index'])\n",
    "        \n",
    "    # send to GPU\n",
    "    subgraph = subgraph.to(device)\n",
    "    pos_graph = pos_graph.to(device)\n",
    "    neg_graph = neg_graph.to(device)\n",
    "\n",
    "    # get node embeddings\n",
    "    node_embeddings = model.forward(subgraph)\n",
    "\n",
    "    # compute score from decoder\n",
    "    pos_scores, neg_scores = model.decoder(subgraph, pos_graph, neg_graph, node_embeddings)\n",
    "\n",
    "    # compute loss\n",
    "    loss, metrics = model.compute_loss(pos_scores, neg_scores)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph.num_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340226"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph.num_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new sampler with fixed subgraph size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get node types\n",
    "node_types = train_neuroKG.ntypes\n",
    "\n",
    "# for each node type, select five random indices from graph, and construct dict\n",
    "original_seed_nodes = {}\n",
    "for node_type in node_types:\n",
    "    node_subset = train_neuroKG.nodes(node_type)\n",
    "    total_elements = node_subset.numel()\n",
    "\n",
    "    num_to_sample = 5\n",
    "    # # get number to sample\n",
    "    # if node_type == 'gene/protein':\n",
    "    #     num_to_sample = 1\n",
    "    # else:\n",
    "    #     num_to_sample = 0\n",
    "\n",
    "    random_indices = torch.randperm(total_elements)[:num_to_sample]\n",
    "    original_seed_nodes[node_type] = node_subset[random_indices]\n",
    "\n",
    "seed_nodes = original_seed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# fanouts = [4, 4, 4]\n",
    "# fixed_k = 10\n",
    "\n",
    "# # define empty dictionary to store reached nodes\n",
    "# all_reached_nodes = [seed_nodes]\n",
    "\n",
    "# # iterate over fanout\n",
    "# for fanout in reversed(fanouts):\n",
    "\n",
    "#     # sample frontier\n",
    "#     frontier = train_neuroKG.sample_neighbors(seed_nodes, fanout, replace=False, prob=None)\n",
    "\n",
    "#     # get reached nodes\n",
    "#     curr_reached = defaultdict(list)\n",
    "#     for c_etype in frontier.canonical_etypes:\n",
    "#         (src_type, rel_type, dst_type) = c_etype\n",
    "#         src, _ = frontier.edges(etype = c_etype)\n",
    "#         curr_reached[src_type].append(src)\n",
    "\n",
    "#     # de-duplication\n",
    "#     curr_reached = {ntype : torch.unique(torch.cat(srcs)) for ntype, srcs in curr_reached.items()}\n",
    "\n",
    "#     # set upper limit\n",
    "#     fixed_k = 20\n",
    "#     upsample_rare_types = True\n",
    "\n",
    "#     # generate type sampling probabilties\n",
    "#     type_count = {node_type: indices.shape[0] for node_type, indices in curr_reached.items()}\n",
    "#     total_count = sum(type_count.values())\n",
    "#     probs = {node_type: count / total_count for node_type, count in type_count.items()}\n",
    "\n",
    "#     # upsample rare node types\n",
    "#     if upsample_rare_types:\n",
    "\n",
    "#         # take scaled square root of probabilities\n",
    "#         prob_dist = list(probs.values())\n",
    "#         prob_dist = np.sqrt(prob_dist)\n",
    "#         prob_dist = prob_dist / prob_dist.sum()\n",
    "\n",
    "#         # update probabilities\n",
    "#         probs = {node_type: prob_dist[i] for i, node_type in enumerate(probs.keys())}\n",
    "\n",
    "#     # generate node counts per type\n",
    "#     n_per_type = {node_type: int(fixed_k * prob) for node_type, prob in probs.items()}\n",
    "#     remainder = fixed_k - sum(n_per_type.values())\n",
    "#     for _ in range(remainder):\n",
    "#         node_type = np.random.choice(list(probs.keys()), p=list(probs.values()))\n",
    "#         n_per_type[node_type] += 1\n",
    "\n",
    "#     # downsample nodes\n",
    "#     curr_reached_k = {}\n",
    "#     for node_type, node_IDs in curr_reached.items():\n",
    "\n",
    "#         # get number of total nodes and number to sample\n",
    "#         num_nodes = node_IDs.shape[0]\n",
    "#         n_to_sample = min(num_nodes, n_per_type[node_type])\n",
    "\n",
    "#         # downsample nodes of current type\n",
    "#         random_indices = torch.randperm(num_nodes)[:n_to_sample]\n",
    "#         curr_reached_k[node_type] = node_IDs[random_indices]\n",
    "\n",
    "#     # update seed nodes\n",
    "#     seed_nodes = curr_reached_k\n",
    "#     all_reached_nodes.append(curr_reached_k)\n",
    "\n",
    "# # merge all reached nodes before sending to DGLGraph.subgraph\n",
    "# merged_nodes = {}\n",
    "# for ntype in train_neuroKG.ntypes:\n",
    "#     merged_nodes[ntype] = torch.unique(torch.cat([reached.get(ntype, []) for reached in all_reached_nodes]))\n",
    "# subg = train_neuroKG.subgraph(merged_nodes, relabel_nodes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
